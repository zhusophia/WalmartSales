---
title: "Weekly Walmart Sales Analysis"
description: |
  Utilizing SQL, R, and Python to analyze Walmart sales.
site: distill::distill_website 
output: 
  distill::distill_article


  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(dplyr)
library(rmarkdown)
library(plotly)
library(ggplot2)
library(distill)
library(kableExtra)
library(RColorBrewer)

```


## Welcome     

This project is currently being re-written. There are major changes in progress. First, I am looking to include more statistical analysis on how sales correlate with every factor, which means that the code and analysis of insights will be updated on a daily basis. Second, I am changing the format of this report to be more similar to a business case study. Thus, sections are being reorganized and contain imperfect writing. Third, because of the following changes, the beautification of charts and tables may not be the main priority. Please proceed with this in mind. 

For a more polished (but less relevant and more informal case study) to showcase my data analysis skills and interest in psychology, please [click here](https://zhusophia.github.io/bigfive/intro.html)! 

## Introduction

This project focuses on analyzing raw data from [this dataset](https://www.kaggle.com/datasets/mikhail1681/walmart-sales?resource=download), which tracks the weekly sales of 45 Walmart stores over 2 years! It also tracks external factors like whether the week contained a holiday, the CPI, and the unemployment rate of the region.

First, the data is cleaned using pandas as we want to make sure that this data is easy to deal with. The primary goal of cleaning is to standardize the data by rounding values and removing missing values. 

```{python cleaning, eval=FALSE, echo=T, code_folding="Show Python code"}
#import 
import pandas as pd
df = pd.read_csv('Walmart_sales.csv', parse_dates=['Date'], dayfirst=False)

#sort by store number and then date
df.sort_values(by=['Store', 'Date'])

#standarize dates to mm-dd-yyyy
df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y').dt.strftime('%m-%d-%Y')

#rounding data 
df.round({'Weekly_Sales':2, 'Temperature':0, 'Fuel_Price':2, 'CPI':3, 'Unemployment':3})

#removing missing values
df.dropna

#create cleaned csv
df.to_csv('./WalmartSalesClean.csv')

```


After cleaning, the data appears like so: 

```{r load data2}
data <- read.csv("WalmartSalesClean.csv")
```

```{r general1 data}
paged_table(head(data))

```

## Analysis
Now that the data has been cleaned, we can use SQL and R to explore this data.

### How do weekly sales vary by store and by year? 

First, weekly sales data is analyzed without taking the other factors into consideration.

We can identify the stores with the highest sales: 


```{sql, storessalessql, eval=F, code_folding="Show SQL code"}
SELECT Store, sum(CAST(Weekly_Sales AS FLOAT)) AS Weekly_Sales
FROM WalmartSalesClean
GROUP BY Store
ORDER BY Weekly_Sales
```

```{r, salesstorechart}

salesstore <- data %>%
  select(Store, Weekly_Sales) %>%
  group_by(Store) %>%
  summarize(Weekly_Sales = sum(Weekly_Sales)/1000000) %>%
  arrange(desc(Weekly_Sales))

salesstorep <- ggplot(data=salesstore, aes(x=Store, y=Weekly_Sales), fill=Store) +
  geom_bar(stat="identity", fill="#0071CE", aes(text = paste0(
    "Store Number: ", Store,
    "<br>",
    "Total Sales (Millions): ", round(Weekly_Sales, digits=3)))) + 
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
  ggtitle("Total Sales per Store") +
  xlab("Store Number") +
  ylab("Total Sales (Millions)") + 
  theme(
    plot.title = element_text(vjust=3, size=16), 
    axis.title.x = element_text(vjust =-3), 
    axis.title.y = element_text(vjust =5)
  )  
ggplotly(salesstorep, tooltip = "text")

```

It seems like store 33, 44, and 5 have the least amount of sales, while store 20, 4, and 14 have the most. 

```{sql, eval=FALSE, code_folding="Show SQL code"}
SELECT TOP 3 Store, sum(Weekly_Sales) AS Weekly_Sales, avg(Temperature) AS Temperature, 
avg(CPI) AS CPI, avg(Fuel_Price) AS Fuel_Price, avg(Unemployment) AS Unemployment
FROM WalmartSalesClean
GROUP BY Store
ORDER BY Weekly_Sales
```

```{r, mostsales}
msales <- data %>%
  group_by(Store) %>%
  summarize(Weekly_Sales = sum(Weekly_Sales), Temperature = mean(Temperature), Fuel_Price = mean(Fuel_Price), CPI = mean(CPI), Unemployment =  mean(Unemployment)) %>%
  filter(Store == 5 | Store == 44 | Store == 33) %>% 
  arrange(desc(Weekly_Sales))

lsales <- data %>%
  group_by(Store) %>%
  summarize(Weekly_Sales = sum(Weekly_Sales), Temperature = mean(Temperature), Fuel_Price = mean(Fuel_Price), CPI = mean(CPI), Unemployment =  mean(Unemployment)) %>%
  filter(Store == 20 | Store == 4 | Store == 14) %>% 
  arrange(Weekly_Sales)

  kable(caption = "Highest Unemployment", msales) %>% kable_styling()
```

```{sql, eval=FALSE, code_folding="Show SQL code"}
SELECT TOP 3 Store, sum(Weekly_Sales) AS Weekly_Sales, avg(Temperature) AS Temperature, 
avg(CPI) AS CPI, avg(Fuel_Price) AS Fuel_Price, avg(Unemployment) AS Unemployment
FROM WalmartSalesClean
GROUP BY Store
ORDER BY Weekly_Sales desc
```

```{r, leastsaletable}
  kable(caption = "Lowest Unemployment", lsales) %>% kable_styling()
```

However, there doesn't seem to be an easily identifiable correlation between sales and the factors (e.g. unemployment, temperature, etc.). Further analysis must be conducted to determine the relationship between these variables.  

We can also see how Walmart sales varied from month to month. 

```{r, salesyeartable}
salesyear <- data %>%
  mutate(Month = substr(Date, 1, 2), Year = substr(Date, 7, 10), Date = paste(Year,"/" ,Month)) %>%
  select(Date, Weekly_Sales) %>%
  group_by(Date) %>%
  summarize(Weekly_Sales = sum(Weekly_Sales))

salesyearp <-ggplot(data=salesyear, aes(x=Date, y=Weekly_Sales/1000000, group=1)) +
  geom_line(colour = "#ffc220")+
  geom_point(aes(text = paste0(
    "Date: ", Date,
    "<br>",
    "Monthly Sales (Millions): ", round(Weekly_Sales/1000000, digits=3))), color = "#0071CE") + 
  theme(axis.text.x = element_text(angle = 90)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
  ggtitle("Total Sales by Month") +
  xlab("Date") +
  ylab("Monthly Sales (Millions)") + 
  theme(
    plot.title = element_text(vjust=3, size=16), 
    axis.title.x = element_text(vjust =-2), 
    axis.title.y = element_text(vjust =4)
  ) 

ggplotly(salesyearp, tooltip = "text")

```
There always seems to be a spike in December and a low in January. The high in December is likely caused by the holiday season, as consumers commonly buy large quantities of food, drinks, and gifts. The low in January could be explained by people saving because of New Year's resolutions or 'No-Spend' January -- trends that emphasize saving money. Consumers may also have less money after the December spending. 

This graph also emphasizes the cyclic nature of consumer spending. The pattern is very consistent across the two years. 

### Do holidays generate more sales? Which holidays affect weekly sales the most?

Holidays are notorious for driving sales since Walmart often runs promotional markdown events to incentivize purchases when consumers are celebrating. Clearly, it works. 

```{sql, eval=FALSE, code_folding="Show SQL code"}
SELECT Holiday_Flag, avg(Weekly_Sales) AS Weekly_Sales
FROM WalmartSalesClean
GROUP BY Holiday_Flag
```

```{r holidaysales}

holidaysales <- data %>% 
  select(Weekly_Sales, Holiday_Flag) %>%
  group_by(Holiday_Flag) %>%
  summarize(Weekly_Sales = mean(Weekly_Sales/1000000))

holidaysales$Holiday_Flag = c("No Holiday", "Holiday")

holidaysalesp <- ggplot(data=holidaysales, aes(x=Holiday_Flag, y=Weekly_Sales), fill=Holiday_Flag) +
  geom_bar(stat="identity", fill="#0071CE", aes(text = paste0(
    "Average Sales (Millions): ", round(Weekly_Sales, digits=3)))) + 
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
  ggtitle("Average Weekly Sales, With and Without Holidays") +
  ylab("Total Sales (Millions)") + 
  xlab("") + 
  theme(
    plot.title = element_text(vjust=3, size=16), 
    axis.title.y = element_text(vjust =5)
  ) 

ggplotly(holidaysalesp, tooltip = "text")


```

Holidays can be narrowed into a few specific categories. When looking at the data, the below weeks are deemed 'Holiday Weeks'. 

```{sql holidaydate, eval=FALSE, code_folding = "Show SQL code"}
SELECT Holiday_Flag, avg(CAST(Weekly_Sales AS FLOAT)) AS Weekly_Sales
FROM WalmartSalesClean
GROUP BY Holiday_Flag
```

```{r, holiday}
holidaydate <- data %>%
  filter(Store == 1 & Holiday_Flag == 1) %>%
  select(Date, Weekly_Sales) 

paged_table(holidaydate)
```

The (repeating) holidays that appear in this data are: 

- Super Bowl (Feb 9th)
- Labor Day (Sept 10th)
- Thanksgiving (Nov 26th)  
- Christmas and Hanukkah (Dec 31th)  

With these holidays, it is implied that the curator of data would have been American and this data could have been taken from American stores. However, the geographical location of these stores are not contained within the data set, so we should be wary of this assumption.    
  
First, the days had the highest profit should be examined:

```{sql test1, eval=FALSE, echo=T, code_folding="Show SQL code"}
ALTER TABLE WalmartSalesClean
ALTER COLUMN Holiday_Flag Int;

 --filtering for holiday flags and sorting 
SELECT Date, SUM(CAST(Weekly_Sales AS FLOAT)) AS Weekly_Sales
FROM WalmartSalesClean 
WHERE Holiday_Flag='1'
GROUP BY Date
ORDER BY Weekly_Sales DESC;
```

```{r, holidayordering, eval}
holidayordering <- data %>% 
  filter(Holiday_Flag == 1) %>%
  select(Date, Weekly_Sales) %>%
  group_by(Date) %>%
  arrange(desc(Weekly_Sales))

paged_table(holidayordering)


```

It seems like Thanksgiving (for all stores) consistently ranks the highest amongst all the holidays. To be more accurate, the total sales per holiday can be extracted. 

```{sql filterholiday, eval=FALSE, echo=T, code_folding="Show SQL code"}
SELECT 
CASE 
WHEN Holiday = 02 THEN 'Super Bowl' --changing months to holiday names
WHEN Holiday = 09 THEN 'Labor Day'
WHEN Holiday = 11 THEN 'Thanksgiving'
WHEN Holiday = 12 THEN 'Christmas'
END AS Holiday, SUM(Weekly_Sales) AS Weekly_Sales
	FROM (  --subquery to have two GROUP BY statements 
	SELECT SUBSTRING (DATE, 1, 2) AS Holiday, SUM(CAST(Weekly_Sales AS FLOAT)) AS Weekly_Sales 
	FROM WalmartSalesClean 
	WHERE Holiday_Flag='1'
	GROUP BY Date
	) AS temptable 
GROUP BY Holiday --Summarizing and reordering data 
ORDER BY Weekly_Sales DESC
```

```{r, monthholidayordering}
holidaydisplay <- data %>% 
  filter(Holiday_Flag == 1) %>%
  mutate(Holiday = substr(Date, 1, 2)) %>%
  select(Holiday, Weekly_Sales) %>%
  group_by(Holiday) %>%
  summarize(Weekly_Sales = sum(Weekly_Sales)) %>%
  arrange(desc(Weekly_Sales))

holidaydisplay$Holiday <- c("Super Bowl", "Labor Day", "Thanksgiving", "Christmas")
```
\n

```{r, holidaypiesales}

holidaypie <- plot_ly(holidaydisplay, labels = ~Holiday, values = ~Weekly_Sales, type = 'pie', 
               textposition = 'inside',
              textinfo = 'label+percent',
                insidetextfont = list(color = '#FFFFFF'),
               hoverinfo = 'text',
        text = ~paste(Holiday, "\n", round(Weekly_Sales/1000000, digits = 3), ' million sales'), 
        marker = list(colors = c('#0071CE', '#ffc220', '#041f41', '#6cace4'))) %>% 
  layout(title = list(text='Total Sales by Holiday', xanchor = "center", font = list(size=24)),
         xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
         yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE), 
        showlegend = F, 
        margin = list(l = 20, r = 20, t = 50))

holidaypie

```

Despite Thanksgiving dominating the upper echelons of highest individual store profit, it seems like people spend the most on the Super Bowl! This could be caused by the cultural significance of Thanksgiving, where it "symbolizes intercultural peace, America's opportunity for newcomers, and the sanctity of home and family".^[https://www.britannica.com/topic/Thanksgiving-Day] However, I am more inclined to believe that Thanksgiving is typically used as an opportunity to invite all your friends and family and spend the entire day cooking -- that means food costs can balloon quickly! 

### Which stores have the lowest and highest unemployment rate?

```{sql test2, eval=FALSE, echo=T, code_folding="Show SQL code"}
-- highest unemployment rate 
SELECT TOP 6 Store, avg(CAST(Unemployment AS decimal)) AS Unemployment, avg(CAST(Weekly_Sales AS decimal)) AS Weekly_Sales
FROM WalmartSalesClean
GROUP BY Store
ORDER BY Unemployment desc, Weekly_Sales desc -- need to use weekly sales as a parameter as stores may have the same unemployment rate

```

```{r employmenttable, echo=F}
highemp <- data %>%
  select(Store, Unemployment, Weekly_Sales) %>%
  group_by(Store) %>%
  summarize(Unemployment = mean(Unemployment), Weekly_Sales = mean(Weekly_Sales)) %>%
  arrange(desc(Unemployment)) 

knitr::kable(head(highemp), caption = "Highest Unemployment") %>%
  kable_styling(position = "center")


```

```{sql lowemploy, eval=FALSE, echo=T, code_folding="Show SQL code"}
-- lowest unemployment rate
SELECT TOP 6 Store, avg(CAST(Unemployment AS decimal)) AS Unemployment, avg(CAST(Weekly_Sales AS decimal)) AS Weekly_Sales
FROM WalmartSalesClean
GROUP BY Store
ORDER BY Unemployment, Weekly_Sales desc

```

```{r lowemptable, echo=F}

lowemp <- data %>%
  select(Store, Unemployment, Weekly_Sales) %>%
  group_by(Store) %>%
  summarize(Unemployment = mean(Unemployment), Weekly_Sales = mean(Weekly_Sales)) %>%
  arrange(Unemployment)

knitr::kable(head(lowemp), caption = "Lowest Unemployment") %>%
  kable_styling(position = "center")


```

Interestingly, there's not a strong correlation between high unemployment and low weekly sales. I'd expect that places with higher unemployment meant that consumers had less disposable income to spend at Walmart. But, Walmart is known for having super cheap products and consumers could've primarily bought inelastic goods and other necessities. 

It could also be that Walmart's penetration pricing worked *too* well. Walmart is notorious for significantly impacting the local economy of the neighborhood, positively and negatively. On one hand, low prices means that consumers can afford more things in a one-stop location, and stores near the Walmart get more business due to the influx of foot traffic. On the other, Walmart is known for hiring many workers and paying minimum wage, as well as displacing small businesses due to their low prices, and subsequently, low profit margins. Only large corporations could offer these low margins as they have the financial resources to temporarily suffer short-term losses in hopes of long-term gains. Local businesses simply can't afford to compete.  

Evidently, the positive and negative effects differ depending on the neighborhood that Walmart enters, with this data being a good example of the impacts that Walmart can have. 

It should also be noted that this data comes from 2010 - 2012 -- aftermath of the Great Recession. Pre-recession, unemployment was at 4.7% in America. However, between 2010 - 2012, unemployment was at a high of 9.8% in January of 2009 and decreased until reaching a low of 7.7% in November 2012.^[https://www.bls.gov/charts/employment-situation/civilian-unemployment-rate.htm] 

### Temperature and Weekly Sales 

Surprisingly, the relationship between temperature and weekly sales in this study contradict recent research papers, as there seems there is no correlation between temperature and sales. 

```{sql, temp, eval=FALSE, code_folding="Show SQL code"}
SELECT Temperature, avg(Weekly_Sales) AS Weekly_Sales
FROM WalmartSalesClean
GROUP BY Temperature 
ORDER BY Weekly_Sales desc
```

```{r, temperaturesales}
tempandsales <- data %>% 
  select(Temperature, Weekly_Sales) %>%
  group_by(Temperature = round(Temperature, digits = 1)) %>%
  summarize(Weekly_Sales = mean(Weekly_Sales/1000000)) %>%
  arrange(desc(Weekly_Sales))


tempsalesp <- ggplot(data=tempandsales, aes(x=Temperature, y=Weekly_Sales)) +
  geom_point(color="#0071CE", aes(text = paste0(
    "Average Weekly Sales (Millions): ", round(Weekly_Sales, digits=2), "\n", "Temperature: ", Temperature))) + 
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
  ggtitle("Temperature and Average Weekly Sales") +
  ylab("Total Sales (Millions)") + 
  xlab("Temperature (Fahrenheit)") + 
  theme(
    plot.title = element_text(vjust=3, size=16), 
    axis.title.x = element_text(vjust=-2),
    axis.title.y = element_text(vjust =5)
  )

ggplotly(tempsalesp, tooltip="text")


```

Studies done by Muro et. al.^[https://www.ualberta.ca/business/media-library/centres/sor/documents/murrayetal2010-theeffectofweatheronconsumerspending.pdf] show that natural sunlight has a positive correlation with spending by increasing positive affect. Furthemore, Zwebner and Goldenburg^[https://web2-bschool.nus.edu.sg/wp-content/uploads/media_rp/publications/zLb1d1490932501.pdf] say that exposure to physical warmth result in emotional warmth, subsequently increasing product valuation. They also discovered that the 'optimal' temperature for increased spending was 78 degrees Fahrenheit. This misattribution of warmth has been called the 'temperature-premium' effect. 

However, both studies test this against specific products, such as cakes, batteries, M&Ms, Gap T shirts. The discrepancy between research and results could be that the objects that were tested in the studies are not common consumer purchases and would not significantly affect weekly sales. Or, the temperature premium may not apply for all products. Furthermore, the studies always utilized artificial heating tools (thermostats or heating pads) within an indoors location. On the other hand, the Walmart data recorded the outdoors temperature, not the temperature within the store. Lastly, the store atmosphere, influenced by indoors lighting, could potentially play an even larger role in customer purchases than weather or temperature. Mitigating the effect of store atmospheres when conducting studies would be crucial. 

There is lots of further research to be done within this topic. Further research could potentially be conducted in the following topics: 

- Does the temperature-premium effect apply to all products? 
- Do natural temperature increases have the same effect as artificial temperature increases? 
- Does temperature affect different types of stores (e.g. grocery stores, superstores, cafes) differently? 

### CPI and Weekly Sales 

The Consumer Price Index (CPI) tracks the weighted average of prices of a metaphorical basket of consumer goods. The goods within this basket are specially selected items that represent the goods and services that a typical US consumer would consume. By tracking the prices of this basket, consumers can understand inflation -- the increases of prices resulting in a lack of purchasing power. 

As an example, the 2023 CPI basket contained the following weighted components: 

```{R, cpi}

knitr::kable(data.frame(Category = c("Food", "Shelter", "Household operations, furnishings and equipment", "Clothing and footwear", "Transportation", "Health and personal care", "Recreation, educaiton and reading", "Alcoholic beverages, tobacco products, and recreational cannabis", "**Total**"),
             Percentage = c(16.69, 29.15, 13.01, 4.55, 16.90, 5.23, 10.27, 4.20, '**100**')), align="lr") %>%
  kable_styling(position = "center")

```

We can also see the seasonally adjusted Canadian CPI^[This CPI score eliminates seasonal movements, such as events caused by climate and holidays.] since 2022. 

```{R, cpimonth}

knitr::kable(data.frame(Date = c("2024/04", "2024/01", "2023/09", "2023/05", "2023/01", "2022/09","2022/05", "2022/01"),
             CPI = c(160.2, 159.3, 158.6, 156.2, 154.9, 152.8, 151.2, 146.3)), align="lr") %>%
  kable_styling(position = "center")

## https://www.bankofcanada.ca/rates/price-indexes/cpi/ taken from here, is there a way to cite this? 

```

There is a clear relationship of the CPI increasing as time goes on -- an indicator of inflation. 

However, we can compare the change in prices in the same time period: 

```{R, cpifuel}

cpifuel <- data.frame(Date = c("2024/04", "2024/01", "2023/09", "2023/05", "2023/01", "2022/09","2022/05", "2022/01"),
             CPI = c(160.2, 159.3, 158.6, 156.2, 154.9, 152.8, 151.2, 146.3), 
             "Fuel Prices (¢/Litre)" =c(169.76, 145.9, 162.45, 160.62, 153.48, 154.09, 183.99, 153.88))
colnames(cpifuel) <- (c("Date", "CPI", "Fuel Prices (¢/Litre)"))

knitr::kable(cpifuel, align="lcr") %>%
  kable_styling(position = "center")




## https://data.ontario.ca/dataset/fuels-price-survey-information/resource/15b54ed9-f711-45c8-bd87-30eb626f7c0a taken from here, is there a way to cite this? 

```

During this time, the average prices of regular unleaded gasoline in Canada do not follow the same pattern of increasing as time goes on. 

This is because of the price elasticity of demand for fuel. Price elasticity is the percentage change in the quantity demanded of a good or service divided by the percentage change in price and shows how sensitive an item's demand is to changes in price. For example, a demand elasticity of -1 would mean that for every 1% increase in price, the demand of a good would fall by 1%. An elasticity of 0 would mean that the demand does not change, even if the price changes. 

Fuel is an inelastic good, meaning that the change in demand is quite small, relative to the change in price. This is because fuel is a  difficult product to substitute -- gas is most likely the only option you have to fuel your car-- and is necessary for daily transportation.  

Although fuel has always been comparatively inelastic good, recent research shows that fuel is more price elastic than we thought. Previously, a widely cited study by Hughes et. al. in 2008 claimed that the price elasticity for gas was between -0.03 and -0.08, meaning that demand barely changes when prices increase. However, in 2017, Coglianese et. al. found that the price elasticity of gasoline was -0.37, with their findings being corroborated by two other studies. 

The errors in previous studies are likely caused by flawed estimation methods and/or questionable data. However, larger societal shifts may also play a part, where major car companies are pushing more fuel-efficient or electric vehicles and more consumers are using public transit as cities become more urbanized and transit infrastructure improves.^[https://www150.statcan.gc.ca/n1/daily-quotidien/240220/dq240220d-eng.htm] 

Hence, one may conclude that an increase in CPI and fuel prices could result in more Walmart sales. With inflation reducing consumer purchasing power, consumers tend to spend less as their necessities have increased in price. Subsequently, they would be likely to go to Walmart -- a store known for having low prices -- to shop, attempting to keep their spending low, resulting in increased sales for Walmart. A similar effect could occur with the rise of fuel prices. Since fuel has inelastic demand, consumers would need to spend more money on fuel, resulting in less money being spent on other necessities. Hence, consumers would likely opt for cheaper products and shop at Walmart compared to more expensive stores.   

However, in regard to this data set, the following relationships are found. 

```{r, cpi table and chart}

cpit <- data %>% 
  select(Weekly_Sales, CPI) %>%
  group_by(CPI = round(CPI, digits = 2)) %>%
  summarize(Weekly_Sales = mean(Weekly_Sales/1000000))


cpic <- ggplot(data=cpit, aes(x=CPI, y=Weekly_Sales), fill=CPI) +
  geom_point(color="#0071CE", aes(text = paste0(
    "Average Sales (Millions): ", round(Weekly_Sales, digits=3), "\n", "CPI: ", CPI))) + 
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
  ggtitle("Average Weekly Sales by CPI") +
  ylab("Average Sales (Millions)") + 
  xlab("CPI") + 
  theme(
    plot.title = element_text(vjust=3, size=16),
    axis.title.x = element_text(vjust=-2),
    axis.title.y = element_text(vjust =6)
  ) 

ggplotly(cpic, tooltip = "text")


```

This data set does not contain CPI values between ~143 to ~ 180. It is likely because the CPI of stores were never in this range, however, the possibility of having missing data should be considered. 

CPI also seems to be separated into 4 main clusters: 

- 125 to 143, which have average sales of 1.1 million. 
- 180 to 190, which have average sales of 1.5 million. 
- 189 to 200, which have average sales of 0.9 million.
- 200 and above, with average sales generally being scattered around 1 million.

Although there doesn't seem to be a correlation between CPI and weekly sales, it seems like stores with the same CPI tend to perform similarly. 


```{r, fuel price table and chart}

tpit <- data %>% 
  select(Weekly_Sales, Fuel_Price) %>%
  group_by(Fuel_Price = round(Fuel_Price, digits = 3)) %>%
  summarize(Weekly_Sales = mean(Weekly_Sales/1000000))


tpic <- ggplot(data=tpit, aes(x=Fuel_Price, y=Weekly_Sales), fill=Fuel_Price) +
  geom_point(color="#0071CE", aes(text = paste0(
    "Average Sales (Millions): ", round(Weekly_Sales, digits=3), "\n", "Fuel Price: ", Fuel_Price))) + 
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
  ggtitle("Average Weekly Sales by Fuel Price") +
  ylab("Average Sales (Millions)") + 
  xlab("Fuel Price (Dollars / Litre)") + 
  theme(
    plot.title = element_text(vjust=3, size=16),
    axis.title.y = element_text(vjust =6)
  ) 

ggplotly(tpic, tooltip = "text")



```

There is also no correlation between fuel price and total sales. However, this graph does have a strange divide, where stores either make ~0.5 million, or ~0.7 million and above. The cause of this discrepancy is unknown, as it happens a various fuel price points.  

It's clear that the trends do not follow the hypothesized positive relationship between these variables and weekly sales. The most probable answer to this is discrepancy is that the economic changes were not enough to shift consumer spending behavior. For example, consumers may not switch to a different store because it's more convenient for them to go (or not go) to Walmart. Furthermore, economics assumes that all consumers are rational as given a large enough sample, the macro trends of consumer spending will gravitate towards rational behavior. However, the sample within this data set may be too small to observe rational trends. A further extension could be diving into behavioral economics and how it affects this data set. 

### Sample Pearson Correlation 

We can also use statistics to back up these claims by checking the correlation between weekly sales and other factors. This gives us insight into the relationship between each of the variables. 

The sample Pearson correlation coefficient was used to determine whether there was a linear correlation between the two sets of data. The coefficient ranges from $-1$ $\le$ $r_{X,Y}$ $\le$ $1$, where $-1$ means a perfect negative linear relationship (as weekly sales go up, the other variable goes down) and $1$ means a perfect positive linear relationship (as weekly sales go up, the other variable goes up.) 

Subsequently:

- if $r_{X,Y}$ $\ge$ $0$, the variables have a positive relationship.  
- $r_{X,Y}$ $\le$ $0$, the variables have a negative relationship. 

Based on the above analysis, the following relationships are predicted: 

- Weekly sales and holiday: $r_{X,Y}$ $\ge$ $0$ 
- Weekly sales and unemployment: $r_{X,Y}$ $\le$ $0$
- Weekly sales and CPI: $r_{X,Y}$ $\ge$ $0$
- Weekly sales and fuel price: $r_{X,Y}$ $\ge$ $0$
- Weekly sales and temperature: $r_{X,Y}$ $\ge$ $0$ 

The following formula will be used:  

$r_{xy}={\frac {n\sum x_{i}y_{i}-\sum x_{i}\sum y_{i}}{\sqrt{(n\sum x^2 - \sum x \sum x)(n\sum y^2 - \sum y \sum y)}}}$

where $x$ is Weekly_Sales, and $y$ the variable we want to test the correlation of. 

The sample Pearson correlation coefficient calculated is:

```{sql samplepc, eval=FALSE, echo=T, code_folding="Show SQL code"}
-- calculating the Pearson coefficient for weekly sales and unemployment. 
-- Replacing the y value (in this case, unemployment) with the other factors 
-- would yield the coefficient for that specific factor 

WITH rdata AS (
SELECT CAST(Unemployment AS decimal) AS Unemployment, CAST(Weekly_Sales AS decimal) AS Weekly_Sales
FROM WalmartSalesClean
),

rawdata AS ( -- calculating expectation 
SELECT Unemployment AS x, Weekly_Sales AS y, Unemployment*Weekly_Sales AS xy, Unemployment*Unemployment AS xsquared, 
Weekly_Sales*Weekly_Sales AS ysquared	
FROM rdata
),

pdata AS ( -- reorganizing data 
SELECT SUM(x) AS sumx, SUM(y) AS sumy, SUM(xy) as sumxy, SUM(xsquared) as sumx2, SUM(ysquared) as sumy2, COUNT(x) as n
FROM rawdata
)

--- covariance(xy) / (std. dev. x) * (std. dev. y)
SELECT (n*sumxy - sumx * sumy) / (sqrt((n*sumx2 - sumx*sumx)*(n*sumy2 - sumy*sumy))) AS "Pearson's Correlation"
FROM pdata

```
```{r, pctable}
#remove dot in title name 

pctable <- data.frame(Variable = c("Unemploymment", "CPI", "Fuel Prices", "Temperature"), "Correlation Coefficient" = c(-0.101436981393985, -0.0724959823775918, -0.00646895141525588,-0.0637688579328555))

colnames(pctable) <- c("Variable", "Correlation Coefficient")


paged_table(pctable)

```

### Point-Biserial Correlation Coefficient

For Holiday_Flag, we need to use a point-biserial correlation coefficient instead of the Pearson correlation coefficent. This is because Holiday_Flag is a dichotomous variable (values can only be 1 or 0 to symbolize yes/no) compared `Weekly Sales`, which is not a dichotomous variable (values can be any number). 

Although you can use Pearson's with dichotomous variables, the usefulness of the correlation would be debatable since it's not meant to be utilized with dichotomous variables. Hence, it's best to use a point-biserial, as it is equivalent to the Pearson correlation coefficient.^[https://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient]

We will be using the following formula: 

${r_{pb}={\frac {M_{1}-M_{0}}{s_{n}}}{\sqrt {\frac {n_{1}n_{0}}{n^{2}}}}}$  

where:

$M_1$ = Mean weekly sales when the Holiday_Flag = 1  
$M_0$ = Mean weekly sales when the Holiday_Flag = 0  
$s_n$ = Standard deviation of weekly sales  
$n_1$ = Number of data points where Holiday_Flag = 1  
$n_2$ = Number of data points where Holiday_Flag = 0  
$n$ = Total number of data points in the sample  

```{sql, eval=FALSE, code_folding="Show SQL code"}

--grabbing avg weekly sales for holiday / no holiday
WITH meanflag AS ( 
SELECT Holiday_Flag, avg(CAST(Weekly_Sales AS decimal)) AS Avg_Weekly_Sales
FROM WalmartSalesClean
GROUP BY Holiday_Flag
), 

-- grabbing number of holiday / no holiday days
countflag AS ( 
SELECT Holiday_Flag, count(Holiday_Flag) AS n_Holiday_Flag
FROM WalmartSalesClean
GROUP BY Holiday_Flag
),

-- combining it into one table
joinmeancount AS ( 
SELECT meanflag.Holiday_Flag, meanflag.Avg_Weekly_Sales, countflag.n_Holiday_Flag
FROM meanflag
INNER JOIN countflag
	ON countflag.Holiday_Flag = meanflag.Holiday_Flag
),

-- adding standard deviation and total count of weekly sales
joinstdn AS ( 
SELECT STDEV(CAST(Weekly_Sales AS decimal)) AS sn, count(Weekly_Sales) as n, 
(SELECT n_Holiday_Flag FROM joinmeancount WHERE Holiday_Flag = 1) AS n1, 
(SELECT n_Holiday_Flag FROM joinmeancount WHERE Holiday_Flag = 0) AS n0, 
(SELECT Avg_Weekly_Sales FROM joinmeancount WHERE Holiday_Flag = 1) AS m1,
(SELECT Avg_Weekly_Sales FROM joinmeancount WHERE Holiday_Flag = 0) AS m0
FROM WalmartSalesClean
)

-- calculating correlation, need to convert into floats for division
SELECT ((m1-m0)/sn) * SQRT((CAST(n1 * n0 AS FLOAT)) / (CAST(n * n AS FLOAT))) AS r_pb 
FROM joinstdn

```

which results in an $r_{pb}$ value of ~ 0.037 -- a slight positive correlation. 

```{r, pctablefull}
#remove dot in title name 

pctable <- data.frame(Variable = c("Holiday", "Unemploymment", "CPI", "Fuel Prices", "Temperature"), "Correlation Coefficient" = c(0.0368880993675271, -0.101436981393985, -0.0724959823775918, -0.00646895141525588,-0.0637688579328555))

colnames(pctable) <- c("Variable", "Correlation Coefficient")
paged_table(pctable)

```

The correlation coefficient matches with the data observed within the tables in charts. 

### Geographic Data

Using the information in the data set, it may be possible to trace the geographic locations of the stores. 

```{r}

data %>%
  select(Store, Unemployment, CPI) %>%
  group_by(Store) %>%
  summarize(Unemployment = mean(Unemployment), mean(CPI)) %>%
  arrange(Unemployment)



```


<!--

However, 

```{r}

emp23 <- data %>% 
  filter(Store == 23) %>% 
  mutate(Month = substr(Date, 1, 2), Year = substr(Date, 7, 10), Date = paste(Year,"/" ,Month)) %>%
  select(Date, Unemployment) %>%
  group_by(Date) %>%
  summarize(Unemployment = mean(Unemployment))

emp23
  

ggplot(data=emp23, aes(x=Date, y=Unemployment, group=1)) +
  geom_line()+
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90)) 

```

potentially uerope? 
- https://w3.unece.org/PXWeb2015/pxweb/en/STAT/STAT__20-ME__3-MELF/40_en_MEUnRateY_r.px/table/tableViewLayout1/
- https://www150.statcan.gc.ca/n1/pub/75-001-x/2010104/charts-graphiques/11148/c-g000k-eng.htm
- https://data.oecd.org/unemp/unemployment-rate.htm

We can compare it to OECD data -- 

Countries it's located in 
- US, Mexico, Canada, UK, China, South Africa, Argentina, Brazil	

https://www.thestreet.com/markets/history-of-walmart-15092339
https://grocerynews.org/research-rankings/globalization-food-sales
- https://flowingdata.com/2010/04/07/watching-the-growth-of-walmart-now-with-100-more-sams-club/

mention the economy in 2010 - 2012. is this the avg unemployment rate? what was happenign that could've influecned this? 
- how does cpi, temprature, fuel prices affect the data as well? psyhcology? 

Where are these stores located? 

Psychology of Temperature and Buying 
```{r}
tempandsales <- data %>% 
  select(Temperature, Weekly_Sales) %>%
  group_by(Temperature) %>%
  summarize(Weekly_Sales = mean(Weekly_Sales)) %>%
  arrange(desc(Weekly_Sales))

ggplot(data=tempandsales, aes(x=Temperature, y = Weekly_Sales)) +
  geom_bar(stat="identity")

```
- https://www.psychologytoday.com/ca/blog/consumed/201312/warm-and-fuzzy-temperature-and-consumer-behavior
- change labels, make it easier to understand (ex. axis labels, numbers, etc.)
box plot of temperature maybe?
```{r}

```

Pearson's correlation

```{sql test3, eval=FALSE, echo=T}
rawdata AS (
SELECT Unemployment AS x, Weekly_Sales AS y, Unemployment*Weekly_Sales AS xy, Unemployment*Unemployment AS xsquared, Weekly_Sales*Weekly_Sales AS ysquared	
FROM corrdata
),

pdata AS (
SELECT SUM(x) AS sumx, SUM(y) AS sumy, SUM(xy) as sumxy, SUM(xsquared) as sumx2, SUM(ysquared) as sumy2, COUNT(x) as n
FROM rawdata
)

SELECT (n*sumxy - sumx * sumy) / (sqrt((n*sumx2 - sumx*sumx)*(n*sumy2 - sumy*sumy))) AS "Pearson's Correlation"
FROM pdata

```

3. Is there any correlation between CPI and Weekly Sales?  How does the correlation differ when the Holiday Flag is 0 versus when the Holiday Flag is 1?
```{sql test4, eval=FALSE, echo=T}
WITH rdata AS (
SELECT CAST(ROUND(CPI, 0) AS INT) AS CPI, CAST(Weekly_Sales AS decimal) AS Weekly_Sales
FROM WalmartSalesClean
WHERE Holiday_Flag = 0
),

corrdata AS(
SELECT CPI, AVG(Weekly_Sales) AS Weekly_Sales
FROM rdata
GROUP BY CPI
/*ORDER BY CPI ASC*/
),

```

```{sql test5, eval=FALSE, echo=T}
rawdata AS (
SELECT CPI AS x, Weekly_Sales AS y, CPI*Weekly_Sales AS xy, CPI*CPI AS xsquared, Weekly_Sales*Weekly_Sales AS ysquared	
FROM corrdata
),

pdata AS (
SELECT SUM(x) AS sumx, SUM(y) AS sumy, SUM(xy) as sumxy, SUM(xsquared) as sumx2, SUM(ysquared) as sumy2, COUNT(x) as n
FROM rawdata
)

SELECT (n*sumxy - sumx * sumy) / (sqrt((n*sumx2 - sumx*sumx)*(n*sumy2 - sumy*sumy))) AS "Pearson's Correlation"
FROM pdata

```

fuel prices affect on economy: https://investopedia.com/financial-edge/0511/how-gas-prices-affect-the-economy.aspx


  - average weekly sales -> density graph? 
  - what traits could impact that? take the average of cpi and stuff and see if there's a massive difference? 
  - summary of everything you learned would be very useful! 

https://www.kaggle.com/code/phuvd1609/walmart-sales-forecasting   

https://docs.google.com/spreadsheets/d/1JNzGqUo9HEn5WOKftEgcm94JddOK6G3_QW-UI4sFNUY/edit#gid=0

https://coolors.co/d8dbe2-a9bcd0-58a4b0-373f51-1b1b1e

-->
